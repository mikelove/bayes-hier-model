---
title: Bayesian Hierarchical Models
subtitle: Stein Lab meeting 9.4.2019
author: Michael Love
framework: io2012
highlighter: highlight.js
hitheme: solarized_light
widgets: mathjax
mode: selfcontained
knit: slidify::knit2slides
---

<style>
em {
    font-style: italic
}
strong {
    font-weight: bold;
}
</style>

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5, cache=FALSE) 
```

# Start with Bayesian model

What is a likelihood? Suppose:

$$ y_i \sim N(\mu, 1) $$

```{r}
set.seed(1)
y <- rnorm(5, mean=4, sd=1)
```

```{r echo=FALSE, fig.width=9}
rafalib::bigpar(1,2)
plot(y, rep(0,5), xlim=c(0,8), ylim=c(0, 1),
     xlab=expression(mu),
     yaxt="n", col="blue", ylab="likelihood")
abline(h=0, col=rgb(0,0,0,.2))
s <- 0:80/10
for (i in 1:5) {
  lines(s, dnorm(s, y[i], 1), col="blue")
}
plot(y, rep(0,5), xlim=c(0,8), ylim=c(0, 1),
     xlab=expression(mu),
     yaxt="n", col="blue", ylab="likelihood")
abline(h=0, col=rgb(0,0,0,.2))
s <- 0:800/100
z <- exp(rowSums(sapply(y, function(yy) dnorm(s, yy, 1, log=TRUE))))
lines(s, 100*z/sum(z), col="blue")
```

---

# Bayesian model

What is a posterior? Likelihood $\times$ some other distribution

$$ f(\hat{\mu}) \propto \prod_i \left[ \varphi(y_i; \mu=\hat{\mu}, \sigma=1) \right] \times g(\hat{\mu}) $$

```{r echo=FALSE, fig.width=12}
rafalib::bigpar(1,3)
plot(y, rep(0,5), xlim=c(0,8), ylim=c(0, 1),
     xlab=expression(mu),
     yaxt="n", col="blue", ylab="likelihood")
abline(h=0, col=rgb(0,0,0,.2))
s <- 0:80/10
for (i in 1:5) {
  lines(s, dnorm(s, y[i], 1), col="blue")
}
plot(y, rep(0,5), xlim=c(0,8), ylim=c(0, 1),
     xlab=expression(mu),
     yaxt="n", col="blue", ylab="posterior")
abline(h=0, col=rgb(0,0,0,.2))
s <- 0:800/100
z <- exp(rowSums(sapply(y, function(yy) dnorm(s, yy, 1, log=TRUE))))
zz <- dnorm(s, 2, 2) * exp(rowSums(sapply(y, function(yy) dnorm(s, yy, 1, log=TRUE))))
lines(s, 100*z/sum(z), col="blue")
lines(s, dnorm(s, 2, 2), col="red")
lines(s, 100*zz/sum(zz), col="purple")
plot(y, rep(0,5), xlim=c(0,8), ylim=c(0, 1),
     xlab=expression(mu),
     yaxt="n", col="blue", ylab="posterior")
abline(h=0, col=rgb(0,0,0,.2))
s <- 0:800/100
z <- exp(rowSums(sapply(y, function(yy) dnorm(s, yy, 1, log=TRUE))))
zz <- dnorm(s, 6, 1) * exp(rowSums(sapply(y, function(yy) dnorm(s, yy, 1, log=TRUE))))
lines(s, 100*z/sum(z), col="blue")
lines(s, dnorm(s, 6, 1), col="red")
lines(s, 100*zz/sum(zz), col="purple")
```

---

# What is a Bayesian Hierarchical Model?

<center>
<img height=300 src="map.jpg">
</center>

* Like before, Bayesian model for data $y_i$
* But there is some structure in the data
* E.g. suppose we have data on schools, schools are in grouped into regions
* So we now have multiple $\mu$ to estimate (say, one per school), they are related to each other

---

# A simple hierarchical model to start

$$
\begin{align} 
y_i \mid \mu_i &\sim N(\mu_i, \sigma^2=1) \\
\mu_i &\sim N(0, \sigma^2=A) \\
\hat{\mu}_i^{Bayes} \mid A &= \left(1 - \frac{1}{A+1} \right) y_i
\end{align}
$$

> "Stein's Estimation Rule and Its Competitors - An Empirical Bayes Approach."
> Bradley Efron and Carl Morris,
> *Journal of the American Statistical Association*,
> Vol. 68, No. 341 (Mar., 1973), pp. 117-130 (14 pages)
> <https://doi.org/10.2307/2284155>

---

# Let's plug in some numbers

Let's try A = 1 (SD = 1) and A = 9 (SD = 3)

```{r}
mu <- rnorm(50, 0, sqrt(1))
y <- rnorm(50, mu, 1)
```

```{r echo=FALSE, fig.width=12}
rafalib::bigpar(1,2)
rafalib::nullplot(-4,4,.9,2.1, yaxt="n", main="A = 1")
axis(2,1:2,c("Bayes",expression(y[i])),las=2)
segments(y, rep(2,10), (1 - 1/(1 + 1))*y, rep(1,10))
mu <- rnorm(50, 0, sqrt(9))
y <- rnorm(50, mu, 1)
rafalib::nullplot(-8,8,.9,2.1, yaxt="n", main="A = 9")
axis(2,1:2,c("Bayes",expression(y[i])),las=2)
segments(y, rep(2,10), (1 - 1/(9 + 1))*y, rep(1,10))
```

---

# What if A is not know?

*empirical Bayes* = estimate distribution of $\mu_i$ using $y_i$

e.g. consider

$$ \hat{\mu}_i^{James-Stein} = \left(1 - \frac{1}{Z} \right) y_i $$

where

$$ Z = \frac{1}{n-2} \sum_{i=1}^n y_i^2 $$

This can be thought of as estimating A + 1 = observed variance

This makes sense: $y_i = \mu_i + \varepsilon_i$

---

# Let's try this out

\[
\begin{align}
\hat{\mu}_i^{MLE} &= y_i \\ 
\hat{\mu}_i^{naive} &= 0
\end{align}
\]

```{r, fig.width=12, echo=FALSE}
jamesstein <- function(y) {
  n <- length(y)
  z <- 1/(n-2) * sum(y^2)
  (1 - 1/z) * y
}
rafalib::bigpar(1,3)
drawit <- function(A) {
  mse <- t(replicate(100, {
    mu <- rnorm(50, 0, sqrt(A))
    y <- rnorm(50, mu, 1)
    c(MLE=sqrt(mean((mu - y)^2)), JS=sqrt(mean((mu - jamesstein(y))^2)))
  }))
  boxplot(mse, ylim=c(0,max(2,sqrt(A)+.5)), col="wheat", ylab="RMSE", main=paste0("A = ",A))
  abline(h=sqrt(A), col="red", lwd=3)
  abline(h=0, col="grey50")
}
drawit(1)
drawit(9)
drawit(.1)
```

---

# A more complex model

Add another level of hierarchy, e.g. schools ($\mu_i$) clustered in
regions ($\theta_k$). 

$k(i)$ gives the region for school $i$.

\[
\begin{align}
y_i \mid \mu_i &\sim N(\mu_i, 1) \\
\mu_i \mid \theta_{k(i)} &\sim N(\theta_{k(i)}, 1) \\
\theta_k &\sim N(0, 5)
\end{align}
\]

I've purposefully made the regions separate (5 > 1) to show the
utility of this model.

This is not so simple to compute, so we will switch to using a tool
called Stan. There are multiple frameworks for performing MCMC where
one specifies the data, parameters, and model, and then samples are
drawn from the posterior. Stan is very fast.

---

# How does this look in Stan?

<br><br><br>

\[
\begin{align}
y_i \mid \mu_i &\sim N(\mu_i, 1) \\
\mu_i \mid \theta_{k(i)} &\sim N(\theta_{k(i)}, 1) \\
\theta_k &\sim N(0, 5)
\end{align}
\]

```{r, eval=FALSE}
y ~ normal(mu, 1);
mu ~ normal(theta[x], 1);
theta ~ normal(0, 5);
```

---

# The full model in Stan

```{r, eval=FALSE}
data {
  int k;
  int n;
  int x[n];
  vector[n] y;
}
parameters {
  vector[k] theta;
  vector[n] mu;
}
model {
    y ~ normal(mu, 1);
    mu ~ normal(theta[x], 1);
    theta ~ normal(0, 5);
}
```

---

# Model output (n=40, k=4)

This model samples very fast in Stan (hundreds of a second). We can
look at posterior distributions: 

<center>
<img src="post_theta.png">
</center>

---

# Compared to truth


<center>
<img src="theta.png">
</center>

---

# Look at the school level ($\mu_i$):

<center>
<img src="shrink.png">
</center>

---

# Did we do better than MLE?

<center>
<img src="mu.png">
</center>

---

# Did we do better than MLE?

<center>
<img src="mu_err.png">
</center>

---


# Look at the school level ($\mu_i$):

<center>
<img src="shrink.png">
</center>

Under what scenario will Bayes be better than a simpler per-region estimator:

James-Stein $\approx \left(1 - \frac{V_y}{V_\mu + V_y} \right)$ to shrink?

---

# When are Bayesian models useful?

* Importantly, it has to run without errors, not all problems "work"
* Bayes estimators converge to MLE as $n \to \infty$, so small $n$
* Or, small $n$ / higher SNR for *some* groups
* Example: some schools have not much data, some regions have few schools
* Or, sharing information for estimating mid-level parameters, e.g. $V_\mu$

---

# Many Bayesian Hierarchical Models in genomics

* limma, edgeR, DESeq2/apeglm
* These are *empirical Bayes*, don't involve MCMC

<center>
<img src="deseq2.png">
</center>

---

# Bayesian Hierarchical Models in GWAS

* eCAVIAR - estimated regression coefficients multivariate normal
* Zhang, ..., Chatterjee, *Estimation of complex effect-size distributions using summary-level statistics*
  [Nat Gen](https://www.nature.com/articles/s41588-018-0193-x) (2018)

<center>
<img src="chatt1.png"><br>
$\dots$
</center>



<center>
<img src="chatt2.png"><br>
$\dots$ 
</center>


<center>
<img src="chatt3.png">
</center>

---

# Modeling TF binding across cell types

* Love *et al* *Role of the chromatin landscape and sequence in
determining ... binding* [NAR](https://academic.oup.com/nar/article/45/4/1805/2605828) (2017)

<center>
<img src="gr.png">
</center>

---

# Why Bayesian Hierarchical Models for Stein lab datasets?

* MR Locus - 
* pathQTL - 
